{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Pip Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install music21\n",
    "!pip3 install torch torchvision torchaudio\n",
    "!pip3 install numPy\n",
    "!pip3 install pretty_midi\n",
    "!pip3 install seaborn\n",
    "!pip3 install matplotlib\n",
    "!pip3 install pydub\n",
    "!pip3 install midi2audio\n",
    "!pip3 install requests\n",
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import music21\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pretty_midi\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import glob\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from scipy.io.wavfile import write\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "\n",
    "url = 'https://keymusician01.s3.amazonaws.com/FluidR3_GM.zip'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Ensure the request was successful\n",
    "if response.status_code == 200:\n",
    "    with open(\"FluidR3_GM.zip\", 'wb') as f:\n",
    "        f.write(response.content)\n",
    "else:\n",
    "    print(\"Failed to retrieve the file\")\n",
    "\n",
    "with zipfile.ZipFile(\"FluidR3_GM.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"FluidR3_GM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from midi2audio import FluidSynth\n",
    "\n",
    "# Path to your SoundFont file\n",
    "soundfont = 'FluidR3_GM/FluidR3_GM.sf2'\n",
    "\n",
    "# Initialize FluidSynth with the specified SoundFont\n",
    "fs = FluidSynth(soundfont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Maestro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('data/maestro-v2.0.0')\n",
    "if not data_dir.exists():\n",
    "  tf.keras.utils.get_file(\n",
    "      'maestro-v2.0.0-midi.zip',\n",
    "      origin='https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip',\n",
    "      extract=True,\n",
    "      cache_dir='.', cache_subdir='data',\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains about 1,200 MIDI files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob(str(data_dir/'**/*.mid*'))\n",
    "print('Number of files:', len(filenames))\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = filenames[2]\n",
    "print(sample_file)\n",
    "def play_midi(midi_filename, save_name):\n",
    "    \"\"\"\n",
    "    Converts a MIDI file to an audio file and plays it.\n",
    "\n",
    "    Args:\n",
    "    midi_filename (str): The path of the MIDI file to be played.\n",
    "\n",
    "    Returns:\n",
    "    IPython.display.Audio: An audio widget for playing the converted audio in Jupyter notebooks.\n",
    "    \"\"\" \n",
    "    # Convert the MIDI file to a WAV file. The output is saved as 'test.wav'.\n",
    "    fs.midi_to_audio(midi_filename, save_name)\n",
    "\n",
    "    # Return an audio widget that can play the generated WAV file.\n",
    "    return Audio(save_name)\n",
    "play_midi(sample_file, 'test.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_midi_data(midi_file):\n",
    "    \"\"\"\n",
    "    Extracts note data along with instrument information from all instruments in a MIDI file.\n",
    "\n",
    "    Args:\n",
    "    midi_file (str): The path to the MIDI file.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the extracted note data and instrument information.\n",
    "    \"\"\"\n",
    "    # Load the MIDI file\n",
    "    pm = pretty_midi.PrettyMIDI(midi_file)\n",
    "\n",
    "    # Initialize a dictionary to hold note data for all instruments\n",
    "    all_notes = collections.defaultdict(list)\n",
    "\n",
    "    # Process each instrument in the MIDI file\n",
    "    for instrument in pm.instruments:\n",
    "        # Extract instrument information\n",
    "        program = instrument.program\n",
    "        is_drum = instrument.is_drum\n",
    "        name = pretty_midi.program_to_instrument_name(program)\n",
    "\n",
    "        # Sort the notes by their start time for sequential processing\n",
    "        sorted_notes = sorted(instrument.notes, key=lambda note: note.start)\n",
    "\n",
    "        # Initialize variable to keep track of the previous note's start time\n",
    "        prev_start = sorted_notes[0].start if sorted_notes else 0\n",
    "\n",
    "        # Iterate over sorted notes to extract data\n",
    "        for note in sorted_notes:\n",
    "            start = note.start\n",
    "            end = note.end\n",
    "            all_notes['pitch'].append(note.pitch)          # Pitch of the note\n",
    "            all_notes['start'].append(start)               # Start time of the note\n",
    "            all_notes['end'].append(end)                   # End time of the note\n",
    "            all_notes['step'].append(start - prev_start)   # Time since the start of the previous note\n",
    "            all_notes['duration'].append(end - start)      # Duration of the note\n",
    "            all_notes['velocity'].append(note.velocity)    # Velocity of the note\n",
    "\n",
    "            # Add instrument information to each note\n",
    "            all_notes['program'].append(program)           # Program number of the instrument\n",
    "            all_notes['is_drum'].append(is_drum)           # Drum status of the instrument\n",
    "            all_notes['instrument_name'].append(name)      # Name of the instrument\n",
    "\n",
    "            prev_start = start\n",
    "\n",
    "    # Convert the all_notes dictionary to a DataFrame and return\n",
    "    return pd.DataFrame({name: np.array(value) for name, value in all_notes.items()})\n",
    "\n",
    "# Example usage\n",
    "midi_info = extract_midi_data(sample_file)\n",
    "print(midi_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_df = midi_info.iloc[::-1].reset_index(drop=True)\n",
    "print(reversed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_instrument(filenames):\n",
    "    \"\"\"\n",
    "    Counts the number of MIDI files that contain more than one instrument.\n",
    "\n",
    "    Args:\n",
    "    filenames (list of str): A list of paths to MIDI files.\n",
    "\n",
    "    Returns:\n",
    "    int: The number of MIDI files with more than one instrument.\n",
    "    \"\"\"\n",
    "    total = 0  # Correct initialization of the total count\n",
    "    for midi_file in filenames:\n",
    "        # Check if the number of instruments in the file is more than one\n",
    "        pm = pretty_midi.PrettyMIDI(midi_file)\n",
    "        if len(pm.instruments) > 1:\n",
    "            total += 1\n",
    "    return total\n",
    "print(count_instrument(filenames[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_note(note, include_velocity=True):\n",
    "    \"\"\"\n",
    "    Plot a single MIDI note.\n",
    "    \n",
    "    Args:\n",
    "    note (pretty_midi.Note): The note to plot.\n",
    "    include_velocity (bool): Whether to include velocity in color intensity.\n",
    "    \"\"\"\n",
    "    x = note.start\n",
    "    y = note.pitch\n",
    "    width = note.end - note.start\n",
    "    height = 1\n",
    "    if include_velocity:\n",
    "        color = (note.velocity / 127, 0, 1 - note.velocity / 127)\n",
    "    else:\n",
    "        color = 'blue'\n",
    "    plt.gca().add_patch(plt.Rectangle((x, y), width, height, edgecolor='none', facecolor=color))\n",
    "\n",
    "def plot_midi(midi_file, include_drum=False, figure_size=(12, 6), color_map=None):\n",
    "    \"\"\"\n",
    "    Plots a MIDI file using pretty_midi.\n",
    "    \n",
    "    Args:\n",
    "    midi_file (str): The path to the MIDI file.\n",
    "    include_drum (bool): Whether to include drum instruments in the plot.\n",
    "    figure_size (tuple): The size of the figure.\n",
    "    color_map (function): A function to determine the color of the notes based on their properties.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
    "    except IOError:\n",
    "        print(f\"Error loading {midi_file}. Please check the file path.\")\n",
    "        return\n",
    "    except ValueError:\n",
    "        print(f\"Invalid MIDI file: {midi_file}.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=figure_size)\n",
    "\n",
    "    for instrument in midi_data.instruments:\n",
    "        if instrument.is_drum and not include_drum:\n",
    "            continue\n",
    "\n",
    "        for note in instrument.notes:\n",
    "            if color_map:\n",
    "                plot_single_note(note, include_velocity=False)\n",
    "                plt.gca().patches[-1].set_facecolor(color_map(note))\n",
    "            else:\n",
    "                plot_single_note(note)\n",
    "\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Pitch')\n",
    "    plt.title('MIDI Visualization')\n",
    "    plt.grid(True)\n",
    "    plt.xlim(0, midi_data.get_end_time())\n",
    "    plt.ylim(0, 128)  # MIDI pitches range from 0 to 127\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_midi(sample_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi_from_dataframe(midi_info, output_file):\n",
    "    \"\"\"\n",
    "    Creates a MIDI file from a DataFrame containing MIDI data for multiple instruments.\n",
    "\n",
    "    Args:\n",
    "    midi_info (pd.DataFrame): The DataFrame containing MIDI data.\n",
    "    output_file (str): The path where the new MIDI file will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a new PrettyMIDI object\n",
    "    new_midi = pretty_midi.PrettyMIDI()\n",
    "\n",
    "    # Group the DataFrame by instrument attributes\n",
    "    grouped = midi_info.groupby(['program', 'is_drum', 'instrument_name'])\n",
    "\n",
    "    # Iterate over each group (instrument) and create a track for each\n",
    "    for (program, is_drum, instrument_name), group in grouped:\n",
    "        # Create a new instrument\n",
    "        new_instrument = pretty_midi.Instrument(program=program, is_drum=is_drum, name=instrument_name)\n",
    "\n",
    "        # Iterate over the rows in the group to create notes\n",
    "        for index, row in group.iterrows():\n",
    "            # Create a new Note object from the row data\n",
    "            note = pretty_midi.Note(\n",
    "                velocity=int(row['velocity']),\n",
    "                pitch=int(row['pitch']),\n",
    "                start=row['start'],\n",
    "                end=row['end']\n",
    "            )\n",
    "            # Add the note to the instrument\n",
    "            new_instrument.notes.append(note)\n",
    "\n",
    "        # Add the instrument to the PrettyMIDI object\n",
    "        new_midi.instruments.append(new_instrument)\n",
    "\n",
    "    # Write to a MIDI file\n",
    "    new_midi.write(output_file)\n",
    "\n",
    "# Example usage\n",
    "create_midi_from_dataframe(midi_info, 'test_new.midi')\n",
    "# Assuming play_midi is a function you have defined to play MIDI files\n",
    "play_midi('test_new.midi')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = filenames[0]\n",
    "tail = filenames[1]\n",
    "\n",
    "head_midi = extract_midi_data(head)\n",
    "tail_midi = extract_midi_data(tail).iloc[::-1].reset_index(drop=True)  # reverse\n",
    "\n",
    "# Ensure both DataFrames have the same length\n",
    "min_length = min(len(head_midi), len(tail_midi))\n",
    "\n",
    "head_midi = head_midi.iloc[:min_length].reset_index(drop=True)\n",
    "tail_midi = tail_midi.iloc[:min_length].reset_index(drop=True)\n",
    "key_order = ['pitch', 'step', 'duration', 'velocity']\n",
    "\n",
    "print(head_midi.shape)\n",
    "print(tail_midi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_instrument(midi_info):\n",
    "    train_notes = np.stack([midi_info[key] for key in key_order], axis=1)\n",
    "    notes_ds = tf.data.Dataset.from_tensor_slices(train_notes)\n",
    "    return notes_ds\n",
    "head_midi_single = one_instrument(head_midi)\n",
    "tail_midi_single = one_instrument(tail_midi)\n",
    "\n",
    "print(head_midi_single.element_spec)\n",
    "print(tail_midi_single.element_spec)\n",
    "\n",
    "for element in tail_midi_single:\n",
    "    for ele in element:\n",
    "        print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(dataset: tf.data.Dataset, seq_length: int, vocab_size=128) -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Converts a dataset of individual notes into a dataset of note sequences for model training.\n",
    "\n",
    "    Args:\n",
    "    dataset (tf.data.Dataset): The original dataset, where each element is a single note.\n",
    "    seq_length (int): The length of the sequences to be created.\n",
    "    vocab_size (int): The size of the vocabulary, used for normalization. Default is 128.\n",
    "\n",
    "    Returns:\n",
    "    tf.data.Dataset: A TensorFlow Dataset containing sequences of notes, with each sequence paired with a label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extend the sequence length to include labels\n",
    "    seq_length = seq_length + 1\n",
    "\n",
    "    # Create overlapping windows of the specified sequence length\n",
    "    windows = dataset.window(seq_length, shift=1, stride=1, drop_remainder=True)\n",
    "\n",
    "    # Flatten the windows into sequences\n",
    "    flatten = lambda x: x.batch(seq_length, drop_remainder=True)\n",
    "    sequences = windows.flat_map(flatten)\n",
    "    \n",
    "    # Function to normalize the pitch of the notes\n",
    "    def scale_pitch(x):\n",
    "        # Normalize only the pitch values, assuming they are in the first position\n",
    "        x = x / [vocab_size, 1.0, 1.0, vocab_size]\n",
    "        return x\n",
    "\n",
    "    # Function to split each sequence into inputs and labels\n",
    "    def split_labels(sequences):\n",
    "        # The inputs are all but the last note in the sequence\n",
    "        inputs = sequences[:-1]\n",
    "        print(inputs)\n",
    "        # The label is the last note in the sequence\n",
    "        labels_dense = sequences[-1]\n",
    "\n",
    "        # Creating a dictionary of labels for each feature\n",
    "        labels = {key: labels_dense[i] for i, key in enumerate(key_order)}\n",
    "        return scale_pitch(inputs), labels\n",
    "    \n",
    "    # Apply the splitting and scaling to each sequence\n",
    "    return sequences.map(split_labels, num_parallel_calls=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 25\n",
    "vocab_size = 128\n",
    "head_seq_ds = create_sequences(head_midi_single, seq_length, vocab_size)\n",
    "head_seq_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_seq_ds = create_sequences(tail_midi_single, seq_length, vocab_size)\n",
    "tail_seq_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input shape will be\n",
    " [H1, H2, H3\n",
    "  T1, T2, T3]\n",
    "Target will be\n",
    "[H4, T4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_datasets(ds1, ds2):\n",
    "    \"\"\"\n",
    "    Combines two TensorFlow datasets into one, concatenating their main tensors to shape (50, 4), and also combines their labels.\n",
    "\n",
    "    Args:\n",
    "    ds1 (tf.data.Dataset): The first dataset to combine. Each element is a tuple of (input, label).\n",
    "    ds2 (tf.data.Dataset): The second dataset to combine. Each element is a tuple of (input, label).\n",
    "\n",
    "    Returns:\n",
    "    tf.data.Dataset: A TensorFlow Dataset with combined elements and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def concatenate_elements(element1, element2):\n",
    "        # Extracting the main tensors and labels\n",
    "        tensor1, label1 = element1\n",
    "        tensor2, label2 = element2\n",
    "\n",
    "        # Concatenate inputs along the first axis\n",
    "        combined_tensor = tf.concat([tensor1, tensor2], axis=0)\n",
    "\n",
    "        # Combine labels into a dictionary. Assuming label1 and label2 are structured similarly.\n",
    "        combined_labels = {f'{key}0': label1[key] for key in label1}\n",
    "        combined_labels.update({f'{key}1': label2[key] for key in label2})\n",
    "\n",
    "        return combined_tensor, combined_labels\n",
    "\n",
    "    # Zip the datasets to create pairs of elements\n",
    "    zipped_datasets = tf.data.Dataset.zip((ds1, ds2))\n",
    "\n",
    "    # Map the concatenate function over the dataset\n",
    "    combined_dataset = zipped_datasets.map(concatenate_elements)\n",
    "\n",
    "    return combined_dataset\n",
    "\n",
    "# Example usage\n",
    "combined_dataset = combine_datasets(head_seq_ds, tail_seq_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_dataset(dataset):\n",
    "    total = 0\n",
    "    for elements in dataset:\n",
    "        for element in elements:\n",
    "            print(element)\n",
    "            print(\"==\")\n",
    "        total += 1\n",
    "    return total\n",
    "print(num_dataset(combined_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, labels in combined_dataset.take(1):\n",
    "    print(\"Label keys:\", labels.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_with_positive_pressure(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    mse = (y_true - y_pred) ** 2\n",
    "    positive_pressure = 10 * tf.maximum(-y_pred, 0.0)\n",
    "    return tf.reduce_mean(mse + positive_pressure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (seq_length * 2, 4)  # Combined length of two songs, 4 features each\n",
    "learning_rate = 0.005\n",
    "\n",
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "x = tf.keras.layers.LSTM(128)(inputs)\n",
    "\n",
    "# Flatten the output of LSTM to map it to the dense layers\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "# Outputs for two notes, each with 4 features\n",
    "output_features = ['pitch', 'step', 'duration', 'velocity']\n",
    "outputs = {}\n",
    "for i in range(2):  # Two notes\n",
    "    for feature in output_features:\n",
    "        # Assuming 128 units for 'pitch' and 1 unit for others as per your structure\n",
    "        units = 128 if feature == 'pitch' or feature == 'velocity' else 1\n",
    "        outputs[f'{feature}{i}'] = tf.keras.layers.Dense(units, name=f'{feature}{i}')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Define losses for each output\n",
    "loss = {}\n",
    "\n",
    "for key in outputs:\n",
    "    if 'pitch' in key:\n",
    "        loss[key] = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    if 'velocity' in key:\n",
    "        loss[key] = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    else:\n",
    "        loss[key] = mse_with_positive_pressure \n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = num_dataset(combined_dataset) - seq_length\n",
    "train_dataset = (combined_dataset.shuffle(buffer_size).batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE))\n",
    "losses = model.evaluate(train_dataset, return_dict=True)\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=loss,\n",
    "    loss_weights={\n",
    "        'pitch': 0.05,\n",
    "        'step': 1.0,\n",
    "        'duration':1.0,\n",
    "    },\n",
    "    optimizer=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = model.evaluate(train_dataset, return_dict=True)\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./training_checkpoints/ckpt_{epoch}',\n",
    "        save_weights_only=True),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='loss',\n",
    "        patience=5,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 50\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history['loss'], label='total loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_last_sequence(dataset, seq_length):\n",
    "    last_sequence = None\n",
    "\n",
    "    # Iterate over the dataset to get the last sequence\n",
    "    for sequences, _ in dataset.take(1):\n",
    "        # Ensure that the sequences tensor has the expected number of dimensions\n",
    "        if len(sequences.shape) == 3:\n",
    "            # Extract the last 'seq_length' elements as a single sequence\n",
    "            last_sequence = sequences[:, -seq_length:, :]\n",
    "        elif len(sequences.shape) == 2:\n",
    "            # If there's no batch dimension, add it\n",
    "            last_sequence = sequences[-seq_length:, :]\n",
    "            last_sequence = tf.expand_dims(last_sequence, axis=0)\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected shape of the sequences tensor\")\n",
    "\n",
    "    if last_sequence is None:\n",
    "        raise ValueError(\"No sequences were extracted from the dataset\")\n",
    "\n",
    "    return last_sequence\n",
    "\n",
    "# Usage\n",
    "seq_length = 50  # Make sure this is correctly set to 50\n",
    "last_seq = extract_last_sequence(combined_dataset, seq_length)\n",
    "print(\"Extracted sequence shape:\", last_seq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_input_shape(model: tf.keras.Model):\n",
    "    if not model or not model.layers:\n",
    "        print(\"Model or model layers are not defined.\")\n",
    "        return\n",
    "\n",
    "    # Accessing the first layer of the model\n",
    "    first_layer = model.layers[0]\n",
    "\n",
    "    # Printing the input shape expected by the first layer\n",
    "    # Note: The input shape could be a list in case of multiple input layers\n",
    "    if isinstance(first_layer.input_shape, list):\n",
    "        print(\"Model expects multiple inputs:\")\n",
    "        for shape in first_layer.input_shape:\n",
    "            print(shape)\n",
    "    else:\n",
    "        print(\"Model expected input shape:\", first_layer.input_shape)\n",
    "\n",
    "# Example usage\n",
    "print_model_input_shape(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_note(\n",
    "    notes: tf.Tensor, \n",
    "    model: tf.keras.Model, \n",
    "    temperature: float = 1.0):\n",
    "    \"\"\"\n",
    "    Generates two notes, each as a tuple of (pitch, step, duration, velocity), using a trained sequence model.\n",
    "    \"\"\"\n",
    "    assert temperature > 0\n",
    "\n",
    "    # Get predictions from the model\n",
    "    \n",
    "    predictions = model.predict(notes)\n",
    "    print(predictions)\n",
    "    # Process predictions for two notes\n",
    "    generated_notes = []\n",
    "    for i in range(2):  # Two notes\n",
    "        pitch_logits = predictions[f'pitch{i}'] / temperature\n",
    "\n",
    "        pitch = tf.random.categorical(pitch_logits, num_samples=1)\n",
    "        pitch = tf.squeeze(pitch, axis=-1).numpy()\n",
    "\n",
    "        step = predictions[f'step{i}']\n",
    "        duration = predictions[f'duration{i}']\n",
    "        duration = tf.squeeze(duration, axis=-1)\n",
    "        step = tf.squeeze(step, axis=-1)\n",
    "        step = tf.maximum(0, step)\n",
    "        duration = tf.maximum(0, duration)\n",
    "        velocity_logits = predictions[f'velocity{i}'] / temperature\n",
    "        velocity =  tf.random.categorical(velocity_logits, num_samples=1)\n",
    "        velocity = tf.squeeze(velocity, axis=-1).numpy()\n",
    "#         # Construct the note tuple\n",
    "        note = (int(pitch), float(step), float(duration), int(velocity))\n",
    "        generated_notes.append(note)\n",
    "\n",
    "    return generated_notes[0], generated_notes[1]\n",
    "\n",
    "# # Usage\n",
    "# # Assuming `last_seq` is your input tensor and `model` is your trained Keras model\n",
    "predict_next_note(last_seq, model, temperature=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_until_three_repeats(model, initial_sequence, seq_length, temperature=1.0, max_pairs=50):\n",
    "    note1_sequence = []\n",
    "    note2_sequence = []\n",
    "    current_sequence = initial_sequence\n",
    "    repeat_count = 0\n",
    "    last_pair_matched = False\n",
    "    total_pairs_generated = 0\n",
    "\n",
    "    while repeat_count < 3 and total_pairs_generated < max_pairs:\n",
    "        # Generate next two notes\n",
    "        note1, note2 = predict_next_note(current_sequence, model, temperature)\n",
    "        total_pairs_generated += 1\n",
    "\n",
    "        # Check if the pitch of note1 is the same as the pitch of note2\n",
    "        if note1[0] == note2[0]:  # Comparing pitch of both notes\n",
    "            if last_pair_matched:\n",
    "                repeat_count += 1\n",
    "            else:\n",
    "                repeat_count = 1\n",
    "            last_pair_matched = True\n",
    "        else:\n",
    "            repeat_count = 0\n",
    "            last_pair_matched = False\n",
    "\n",
    "        # Append notes to their respective sequences\n",
    "        note1_sequence.append(note1)\n",
    "        note2_sequence.append(note2)\n",
    "\n",
    "        if repeat_count >= 3:\n",
    "            break  # Exit if the condition is met for three consecutive pairs\n",
    "\n",
    "        # Reshape and concatenate the new notes to match current_sequence\n",
    "        new_notes_tensor = tf.convert_to_tensor([note1, note2], dtype=current_sequence.dtype)\n",
    "        new_notes_tensor = tf.reshape(new_notes_tensor, (1, 2, -1))\n",
    "\n",
    "        # Concatenate along the sequence dimension\n",
    "        new_sequence = tf.concat([current_sequence[:, :seq_length-2, :], new_notes_tensor], axis=1)\n",
    "        current_sequence = new_sequence\n",
    "\n",
    "    return note1_sequence, note2_sequence\n",
    "\n",
    "# Example usage\n",
    "note1_sequence, note2_sequence = generate_until_three_repeats(model, last_seq, seq_length, temperature=1.0, max_pairs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(note1_sequence)\n",
    "print(note2_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_generated_notes_to_df(generated_notes, existing_df, program, is_drum, instrument_name):\n",
    "    \"\"\"\n",
    "    Appends generated notes to an existing DataFrame.\n",
    "\n",
    "    Args:\n",
    "    generated_notes (list of tuples): Generated notes in the format [(pitch, step, duration, velocity), ...].\n",
    "    existing_df (pd.DataFrame): The existing DataFrame to append notes to.\n",
    "    program (int): The MIDI program number.\n",
    "    is_drum (bool): Whether the instrument is a drum.\n",
    "    instrument_name (str): The name of the instrument.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with the generated notes appended.\n",
    "    \"\"\"\n",
    "    # Determine the start time for the first generated note\n",
    "    initial_start = existing_df['end'].iloc[-1] if not existing_df.empty else 0\n",
    "\n",
    "    # Prepare data for new DataFrame\n",
    "    note_data = []\n",
    "    current_start = initial_start\n",
    "    for note in generated_notes:\n",
    "        pitch, step, duration, velocity = note\n",
    "        start = current_start + step\n",
    "        end = start + duration\n",
    "        note_data.append({\n",
    "            'pitch': pitch,\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'step': step,\n",
    "            'duration': duration,\n",
    "            'velocity': velocity,\n",
    "            'program': program,\n",
    "            'is_drum': is_drum,\n",
    "            'instrument_name': instrument_name\n",
    "        })\n",
    "        current_start = end\n",
    "\n",
    "    # Create a DataFrame from generated_notes\n",
    "    new_notes_df = pd.DataFrame(note_data)\n",
    "\n",
    "    # Append new notes to the existing DataFrame\n",
    "    return pd.concat([existing_df, new_notes_df], ignore_index=True)\n",
    "\n",
    "head_appended_df = append_generated_notes_to_df(note1_sequence, head_midi, program=0, is_drum=False, instrument_name='Acoustic Grand Piano')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_generated_notes_to_df(generated_notes, existing_df, program, is_drum, instrument_name):\n",
    "    \"\"\"\n",
    "    Prepends generated notes to an existing DataFrame.\n",
    "\n",
    "    Args:\n",
    "    generated_notes (list of tuples): Generated notes in the format [(pitch, step, duration, velocity), ...].\n",
    "    existing_df (pd.DataFrame): The existing DataFrame to prepend notes to.\n",
    "    program (int): The MIDI program number.\n",
    "    is_drum (bool): Whether the instrument is a drum.\n",
    "    instrument_name (str): The name of the instrument.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with the generated notes prepended.\n",
    "    \"\"\"\n",
    "    # Calculate the total duration of the generated notes\n",
    "    total_duration = sum(note[2] for note in generated_notes)\n",
    "\n",
    "    # Shift the start and end times of the existing notes\n",
    "    existing_df_shifted = existing_df.copy()\n",
    "    existing_df_shifted['start'] += total_duration\n",
    "    existing_df_shifted['end'] += total_duration\n",
    "\n",
    "    # Prepare data for new DataFrame\n",
    "    note_data = []\n",
    "    current_start = 0  # Start from 0 for the first new note\n",
    "    for note in generated_notes:\n",
    "        pitch, step, duration, velocity = note\n",
    "        start = current_start + step\n",
    "        end = start + duration\n",
    "        note_data.append({\n",
    "            'pitch': pitch,\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'step': step,\n",
    "            'duration': duration,\n",
    "            'velocity': velocity,\n",
    "            'program': program,\n",
    "            'is_drum': is_drum,\n",
    "            'instrument_name': instrument_name\n",
    "        })\n",
    "        current_start = end\n",
    "\n",
    "    # Create a DataFrame from generated_notes\n",
    "    new_notes_df = pd.DataFrame(note_data)\n",
    "\n",
    "    # Prepend new notes to the shifted existing DataFrame\n",
    "    return pd.concat([new_notes_df, existing_df_shifted], ignore_index=True)\n",
    "\n",
    "tail_appended_df = append_generated_notes_to_df(note2_sequence, tail_midi.iloc[::-1].reset_index(drop=True), program=0, is_drum=False, instrument_name='Acoustic Grand Piano')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_midi_from_dataframe(head_appended_df, 'head.midi')\n",
    "play_midi('head.midi', 'final_head.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_midi_from_dataframe(tail_appended_df, 'tail.midi')\n",
    "play_midi('tail.midi', 'final_tail.wav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
